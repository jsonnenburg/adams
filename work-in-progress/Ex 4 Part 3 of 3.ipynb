{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAMS Tutorial #4: Neural Networking (NN) Primer\n",
    "## Part 3 of 3\n",
    "\n",
    "This is the final of three notebooks which will cover the foundations of neural networks. Please review the first two notebooks if you haven't yet, as this notebook assumes you understand the first two. In the first notebook, we discussed how the algorithm can be initialized randomly. The second notebook focused on training the network through back propagation. Training is the process in which we update the weights to maximize the fit of our network to training data. Network training is, therefore, equivalent to maximum likelihood estimation in regression analysis. \n",
    "\n",
    "This notebook will explore a different problem that neural networks are well-suited to solve: classification. We will be rewiring the same code form part 2, only this time, our network will classify samples. Differences will be explained as we go.\n",
    "\n",
    "## This notebook's topic: Adapting what we know for a classification network ##\n",
    " 1. Reviewing classification problems\n",
    " 2. Highlighting differences in the algorithm\n",
    " 3. Revising our code to solve a classification problem\n",
    " 4. A cheat sheet for the most basic neural network problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "range_for_demo = np.linspace(-5, 5, 100)\n",
    "\n",
    "import random\n",
    "random.seed(888)  # set seed for reproducibility\n",
    "np.random.seed(888)\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of classification problems\n",
    "\n",
    "There are a few types of problems which require us to label the classes of data. Here is a quick summary:\n",
    "\n",
    "**Binary classification** tries to predict which of two options is most likely. For example, will this customer spend money today (yes/no)? Will rain fall today (yes/no)?\n",
    " - Single dummy: binary target coded as a single binary variable representing the probability of an event occurring or label 1 (eg. one variable for purchase, no purchase is inferred as the inverse)\n",
    " - Two-dummy: one dummy is the probability of the non-event/label 0, the second being the probability of the event/label 1 (eg. one variable for no purchase and one variable for purchase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single dummy approach\n",
    "Y = [1, 0, 1]\n",
    "\n",
    "# two-dummy approach (same information as the single dummy above)\n",
    "Y_0 = [0, 1, 0]\n",
    "Y_1 = [1, 0, 1]  # note that this vector is identical to single dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single label multi-class classification** tries to predict which of 3 or more options are most likely but only ONE class is possible. Eg. Is this image a cat, dog or bird? It is recommended to one-hot encode the target variable.\n",
    "\n",
    "**Multiple label multi-class classification** tries to predict which of 3 or more options are most likely and when multiple classes are possible. Eg. Which of these products will be purchased? It is recommended to one-hot encode the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences in NNs for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Data Generation: Binary Classification\n",
    "\n",
    "We will now need some new data for this example. We are going to generate two variables with conditional impacts on our target variable. There will be two clusters and each variable will have a slightly different mean per cluster. In order to compare this to a real-world example, it would be a situation where we have two distinct groups. In our previous example in the last notebooks on regression, we said that the target could represent the grade on the final exam. This time, we could say that these two groups represented those who passed and didn't pass a test. Again, one of our variables could represent average hours slept per night and the other could be an indicator for if the student completed the assignments in class. What's important here is that we are predicting membership in one of two groups and not a continuous number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000 # generate number of observations\n",
    "k = 2  # number of features\n",
    "\n",
    "XX, Y = make_blobs(n_samples=n, n_features=k, centers=2, random_state=888, cluster_std=1.5)\n",
    "\n",
    "Y = Y.reshape(n,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture\n",
    "For the most part, the network architecture required for binary classification networks is very similar to those required for regression networks. Our input layer has just as many neurons as we have features and we have a hidden layer whose size we can experiment with. We could also experiment with adding more hidden layers. One of the only main differences concerns the size of the output layer. Note that There are specific sizes based on the values of your target variable as explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer size\n",
    "For regression problems, we had a constant output layer size of 1 neuron because we were only predicting one number.\n",
    "\n",
    "For **binary classification** problems where the target is coded as a single dummy variable, you will need 1 node in your output layer. If there are two dummy variables for the target (one dummy for class 0 and one dummy for class 1), you will need 2 nodes. In our example, we have coded everything as a single dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture:\n",
    "inputLayer_size = 2      # number of features in X, in this case we only have 2\n",
    "hiddenLayer_size = 10    # number of hidden layer nodes\n",
    "outputLayer_size = 1     # number of values to predict, in this case, our target is one vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, for the initialization, we will only take the 5th observation in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(XX[5]).reshape((inputLayer_size, 1))\n",
    "X.shape  # This should be kx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = Y[5].reshape((outputLayer_size, 1))\n",
    "y.shape  # since our target is coded as a single dummy variable, we should see (1, 1) here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight initialization\n",
    "As in part 1 and 2, we randomly initialize weights based on the recommendation we received from Glorot and Bengio (2010). The only thing to be careful of as we refit this network for a classification problem is the size of the output layer which we have already discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = np.sqrt(6 / (inputLayer_size + outputLayer_size))  # Recommended weight initialization\n",
    "\n",
    "# Random weight and bias initialization\n",
    "W_0 = np.random.uniform(-limit, limit, (hiddenLayer_size, inputLayer_size))    # input to hidden layer weights\n",
    "W_1 = np.random.uniform(-limit, limit, (outputLayer_size, hiddenLayer_size))  # hidden to output layer weights\n",
    "\n",
    "B_0 = np.ones((hiddenLayer_size,1))  # input to hiden layer biases\n",
    "B_1 = np.ones((outputLayer_size,1))  # hidden to output layer biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.25580764],\n",
       "       [-6.27077088],\n",
       "       [ 6.47240459],\n",
       "       [ 8.81335129],\n",
       "       [-8.51333128]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_hidden = np.dot(W_0, X) + B_0\n",
    "Z_hidden[:5]  # these are the first five nodes' input values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "Remember that you should distinguish your activation functions in two groups:\n",
    "- Hidden layer activations: here you can experiment with any activation function but are encouraged to use the ones that we introduced in part 2.\n",
    "- Final layer activation: this activation needs to suit your output. We mentioned using a linear activation for regression. Below we will discuss good final activations for classification.\n",
    "\n",
    "For the **hidden layer** in our current network, let's take $tanh$ this time. If you are curious about this function and its derivatives, you can find an explanation in part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function that we will use in this scenario and its derivative\n",
    "def tanH(x):\n",
    "    return (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
    "\n",
    "\n",
    "def tanH_derivative(x):\n",
    "    return 1-tanH(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ],\n",
       "       [-0.99999285],\n",
       "       [ 0.99999522],\n",
       "       [ 0.99999996],\n",
       "       [-0.99999992]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = tanH(Z_hidden)\n",
    "H[:5]  # these are the first five nodes' output values (after activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final activation functions for classification problems\n",
    "\n",
    "We talked about a few possible activation functions such as linear, ReLU, leaky ReLU, tanh and sigmoid. We can experiment with any of these if they are not in the final layer. However, we need to pay attention to the final layer's activation because it determines the range of values that will be output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid and binary classification\n",
    "**If your target is a single dummy**, you will need your network to output a value between 0 and 1 as the probability of class 1. The best final activation function for the output layer will be **sigmoid** whose outputs are always squeezed into a probability between 0 and 1. $tanh$ can also be used but note that it ranges between -1 and 1, so you will need to rescale it. If you would like to see a more detailed description of the sigmoid function including its derivation, you can find it in part 2 in this tutorial series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1-s)\n",
    "\n",
    "plt.plot(range_for_demo, sigmoid(range_for_demo), label='f(x)')\n",
    "plt.plot(range_for_demo, sigmoid_derivative(range_for_demo), label=\"f(x)'\")\n",
    "plt.grid(True)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax and binary classification\n",
    "\n",
    "**If your target is two dummy variables**, you'll have 2 output neurons. The first neuron outputs the probability of class 0 and the second outputs the probability of class 1. We want the sum of the probability of both classes to be 1. In this case, we have a final activation function called **softmax** which will make sure the sum of probabilities in the output to be 1.\n",
    "\n",
    "$$ \\text{softmax}(x) = \\frac{e^{x_i}}{\\sum_{j=1} e^{x_j} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the softmax output for derivation\n",
    "\n",
    "This function takes a inputs $X$ as a vector and produces a vector of probabilities which sum to 1. Since we are predicting for classes, this vector will have the dimension of $ n\\_class * 1$.\n",
    "\n",
    "So, in essence, $\\text{softmax}(x)$ does the following:\n",
    "$$ \\text{softmax} \\bigg( \\begin{bmatrix}  x_{1} \\\\  x_{2}\\\\   \\vdots\\\\ x_{n}\\\\ \\end {bmatrix} \\bigg) \\rightarrow\n",
    "\\begin{bmatrix}  s_{1} \\\\  s_{2} \\\\  \\vdots \\\\ s_{n} \\end {bmatrix} $$\n",
    "\n",
    "If we want to derive this function, we will have to derive each output w.r.t. $x_1$ through $x_n$. This will create a Jacobian (square) matrix:\n",
    "$$ \\begin{bmatrix} \\frac{\\partial s_1}{\\partial x_1} & \\frac{\\partial s_1}{\\partial x_2} & \\ldots \\ & \\frac{\\partial s_1}{\\partial x_n} \\\\ \\frac{\\partial s_2}{\\partial x_1} & \\frac{\\partial s_2}{\\partial x_2} \\\\  \\vdots & \\vdots \\\\  \\frac{\\partial s_1}{\\partial x_n} & \\frac{\\partial s_n}{\\partial x_2} & \\ldots\\ & \\frac{\\partial s_n}{\\partial x_n} \\\\ \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving the softmax\n",
    "\n",
    "If we want to derive this function, we would have to use the quotient rule as we have done in many other calculations before.\n",
    "\n",
    "$$\\begin{aligned} &f(x) = \\frac{g(x)}{h(x)} \\\\\n",
    "& f'(x) = \\frac{g'(x)h(x) - g(x)h'(x)}{[h(x)]^2} \\end{aligned}$$\n",
    "\n",
    "Here, $g(x)=e^{x_i}$ and $h(x)=\\sum_{j=1} e^{x_j}$. There are two situations which we will have when we derive with respect to $x_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1: $i=j$ (Jacobian matrix diagonal)\n",
    "$$g(x)=e^{x_j}$$\n",
    "$$ \\frac{\\partial g(x)}{\\partial x_j}=e^{x_j}$$\n",
    "\n",
    "$$h(x)=\\sum_{j=1} e^{x_j}$$\n",
    "$$ \\frac{\\partial h(x)}{\\partial x_j}=e^{x_j}$$\n",
    "\n",
    "So, if we want to find the partial derivatives of the softmax function, we would need to do the following for this case:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& = \\frac{\\partial \\text{softmax}}{\\partial x_j} \\\\\n",
    "& = \\frac{e^{x_j} \\cdot \\sum_{k=1} e^{x_k} - e^{x_j} \\cdot e^{x_j}}{\\big[ \\sum_{k=1} e^{x_k} \\big]^2 } \\\\\n",
    "& = \\frac{e^{x_j} \\big( \\sum_{k=1} e^{x_k} - e^{x_j} \\big) } {\\big[ \\sum_{k=1} e^{x_k} \\big]^2 } \\\\\n",
    "& = \\frac{e^{x_j}} { \\sum_{k=1} e^{x_k} } \\cdot \\frac{\\sum_{k=1} e^{x_k} - e^{x_j}}{ \\sum_{k=1} e^{x_k} } \\\\\n",
    "& = \\text{softmax} (1- \\text{softmax})\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2: $i \\neq j$ (off-diagonal elements)\n",
    "$$g(x)=e^{x_i}$$\n",
    "$$ \\frac{\\partial g(x)}{\\partial x_j}=0$$\n",
    "\n",
    "$$h(x)=\\sum_{j=1} e^{x_j}$$\n",
    "$$ \\frac{\\partial h(x)}{\\partial x_j}=e^{x_j}$$\n",
    "\n",
    "\n",
    "And for this case, the derivative is:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& = \\frac{\\partial \\text{softmax}}{\\partial x_j} \\\\\n",
    "& = \\frac{0 \\cdot \\sum_{k=1} e^{x_k} - e^{x_j} \\cdot e^{x_i}}{\\big[ \\sum_{k=1} e^{x_k} \\big]^2 } \\\\\n",
    "& = - \\frac{e^{x_j}}{\\sum_{k=1} e^{x_k}} \\cdot \\frac{e^{x_i}}{\\sum_{k=1} e^{x_k}} \\\\\n",
    "& = - \\text{softmax} \\cdot \\text{softmax}\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Just like that, we can now use the softmax's gradient to solve a minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we will only plot the function as the derivative is more complex\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "plt.plot(range_for_demo, softmax(range_for_demo), label=\"f(x)\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now let's continue through the first pass of our classification neural network. We will first calculate the input into the final layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.18882511]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_output = np.dot(W_1, H) + B_1  # Sum of next layer of weights*respective node then add bias\n",
    "Z_output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will apply a sigmoid function which will output a value between 0 and 1. This will represent probability of class 1 for the first forward pass. We can infer the probability of class 0 by finding the inverse of this, $1-\\hat y$. We know that this will very likely be optimized through gradient descent shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1007585]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = sigmoid(Z_output)  # Apply activation for output layer's output\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions for Classification\n",
    "\n",
    "### Cross-Entropy\n",
    "Entropy $H(y)$ is a term from Information Theory which signifies the optimal number of bits needed to encode a certain amount of information. $y_c$ is the probability of the nth event, or in our case, class:\n",
    "\n",
    "$$H(y) = \\sum_c y_c \\cdot \\log \\frac{1}{y_c} = -\\sum_c y_c \\cdot \\log y_c$$\n",
    "\n",
    "Now the cross-entropy $H(y,\\hat{y})$ is the number of bits we'll need to encode symbols from $y$ using the wrong tool $\\hat{y}$. Cross-entropy is always bigger or equal to entropy. Mind that $c$ is the number of classes. \n",
    "\n",
    "$$H(y, \\hat{y}) = \\sum_c y_c \\cdot \\log \\frac{1}{\\hat{y}_c} = -\\sum_c y_c \\cdot \\log \\hat{y}_c$$\n",
    "\n",
    "Interestingly enough, the The KL divergence that you have encountered before in BADS (uplift random forest) is simply the difference between cross-entropy and entropy:\n",
    "$$\\mbox{KL}(y~||~\\hat{y})\n",
    "= \\sum_c y_c \\log \\frac{1}{\\hat{y}_c} - \\sum_c y_c \\log \\frac{1}{y_c}\n",
    "= \\sum_c y_c \\log \\frac{y_c}{\\hat{y}_c}$$\n",
    "\n",
    "We would be calculating cross-entropy for every pair of true/estimated probabilities and averaging it over the sample or batch - this will be our loss function *L* that we will ultimately want to minimize (class i, sample j):\n",
    "\n",
    "$$L=-\\frac{1}{n}\\sum_i \\sum_c y_{i,c} \\log(\\hat{y}_{i,c})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary cross-entropy\n",
    "For **binary classification**, we would use the special case of the equation above with 2 classes called binary cross-entropy (note there are other options like hinge loss).\n",
    "\n",
    "$$L=-\\frac{1}{n}\\sum_i \\big( y_{i,1}  \\log(\\hat{y}_{i,1}) + y_{i,2} \\log(\\hat{y}_{i,2}) \\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving the loss function\n",
    "Now, we need to minimize the loss function. Let's derive the binary cross entropy function for a single observation. We will remove the $\\frac{1}{n}$ since it can be divided out in the minimization:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& = \\frac{\\partial L_i}{\\partial w} \\\\\n",
    "& = - \\frac{\\partial}{\\partial w} y_{1}  \\log(\\hat{y}_{1}) + \\frac{\\partial}{\\partial w}  y_{2} \\log(\\hat{y}_{2}) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "At this point, it is important to remember three things at this point. The first is that the true values $y_i$ are just constants. Secondly, the estimates  $\\hat y_i $ are a function determined by the sigmoid, so we use the chain rule on each $log(\\hat y_i)$. Lastly, we also can replace $y_1$ with $y$ and $y_2$ with $1-y$ since we only have two probabilities/possibilities which are complements of each other. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "& = - \\frac{y_{1}}{\\hat{y}_{1}} \\cdot \\frac{\\partial}{\\partial w}\\hat{y}_{1} - \\frac{y_{2}}{\\hat{y}_{2}} \\cdot \\frac{\\partial}{\\partial w}\\hat{y}_{2} \\\\\n",
    "& = - \\frac{y}{\\hat{y}} \\cdot \\frac{\\partial}{\\partial w}\\hat{y} - \\frac{1-{y}}{1-\\hat{y}} \\cdot \\frac{\\partial}{\\partial w} (1-\\hat{y}) \\\\\n",
    "& = - \\frac{y}{\\hat{y}} \\cdot \\frac{\\partial}{\\partial w}\\hat{y} + \\frac{1-{y}}{1-\\hat{y}} \\cdot \\frac{\\partial}{\\partial w} \\hat{y} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "We know that the final equation which determines $\\hat y$ is the final sigmoid activation. So, we can replace $\\hat y$ with the sigmoid and derive it where necessary (full derivation in part 1 of these tutorials).\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& = - \\frac{y}{\\text{sigmoid}} \\cdot \\frac{\\partial}{\\partial w} \\text{sigmoid} + \\frac{1-{y}}{1-\\text{sigmoid}} \\cdot \\frac{\\partial}{\\partial w} \\text{sigmoid}  \\\\\n",
    "& = - \\frac{y}{\\text{sigmoid}} \\cdot \\text{sigmoid}\\cdot (1-\\text{sigmoid}) + \\frac{1-{y}}{1-\\text{sigmoid}} \\cdot (\\text{sigmoid}\\cdot (1-\\text{sigmoid}))  \\\\\n",
    "& = - y \\cdot (1-\\text{sigmoid}) + (1-y) \\cdot \\text{sigmoid} \\\\\n",
    "& = - y + y \\cdot \\text{sigmoid} + \\text{sigmoid}-y \\cdot \\text{sigmoid} \\\\\n",
    "& = \\text{sigmoid}-{y} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "The sigmoid at the end is also the prediction, so that means: $$ = \\text{sigmoid}-{y} = \\hat{y}-{y} $$ The extreme simplicity of this function's derivative is what makes it so ideal for a loss function, especially when the number of calculations starts to grow in the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the binary cros entropy loss function\n",
    "def bce(true, pred):\n",
    "    eps = 1e-50 # small epsilon so that the log never tries to div by 0\n",
    "    return - np.mean(\n",
    "        np.multiply(true, np.log(pred + eps)) + np.multiply((1-true), np.log(1 - pred + eps)))\n",
    "\n",
    "def bce_derivative(pred, true):\n",
    "    return pred - true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the intitial loss for observation 5, the derivative of the loss function and final activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10620364398497029"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = bce(Y[5], y_hat)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the derivatives associated with the loss function and the final activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1007585]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_derivative = bce_derivative(y_hat, Y[5])\n",
    "loss_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09060622]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_activation_derivative = sigmoid_derivative(Z_output)\n",
    "final_activation_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization based on loss\n",
    "To continue, we can just use the code that we had in part 2. We just need to make sure that our chosen activation this time ($tanh$) is subsituted where appropriate. Again, we did not have to choose $tanh$ and you should experiment with other activations to optimize your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first weights (1, 10)\n",
      "Shape of gradient vector (10, 1)\n",
      "Shape of input (2, 1)\n"
     ]
    }
   ],
   "source": [
    "# Update weights furthest back in the network (between hidden and output layer)\n",
    "gradient_HiddenToOutput = np.dot(loss_derivative*final_activation_derivative, np.transpose(H))\n",
    "gradient_HiddenToOutput.shape\n",
    "\n",
    "# Update output layer biases\n",
    "gradient_HiddenToOutput_bias = loss_derivative*final_activation_derivative\n",
    "\n",
    "# Save the error of the output layer\n",
    "pred_errors = loss_derivative * final_activation_derivative\n",
    "\n",
    "# Find gradient for next step for backpropagation: gradient to update weights between hidden and input layer\n",
    "gradient_InputToHidden = np.dot(W_1.T, pred_errors)\n",
    "print(f'Shape of first weights {W_1.shape}')\n",
    "\n",
    "# Next propagation backwards: derivative of the hidden layer output wrt the hidden layer input (tanh derivative)\n",
    "gradient_InputToHidden = gradient_InputToHidden * tanH_derivative(Z_hidden)\n",
    "print(f'Shape of gradient vector {gradient_InputToHidden.shape}')\n",
    "\n",
    "# Derivate of the hidden layer input wrt to the weight matrix connecting the hidden layer to inputs X\n",
    "gradient_InputToHidden = np.dot(gradient_InputToHidden, np.transpose(X))\n",
    "print(f'Shape of input {X.shape}')\n",
    "\n",
    "# Last update: output layer biases\n",
    "gradient_InputToHidden_bias = np.dot(W_1.T, pred_errors) * tanH_derivative(Z_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial weight correction\n",
    "Now that we have calculated our gradients, we can finally perform our first update! This process should be identical to what we saw in part 2. Just as before, we can set a different step size (learning rate) for this initial optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent step\n",
    "learningRate = 0.1  # define some learning rate for first step\n",
    "\n",
    "# Update weights between hidden and output layer (furthest back)\n",
    "W_1 -= learningRate * gradient_HiddenToOutput\n",
    "# Update bias in output layer\n",
    "B_1 -= learningRate * gradient_HiddenToOutput_bias\n",
    "# Update weights between input and hidden layer (furthest forward)\n",
    "W_0 -= learningRate * gradient_InputToHidden\n",
    "# Update bias in hidden layer\n",
    "B_0 -= learningRate * gradient_InputToHidden_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we have completed our first weight and bias update on this journey through neural network optimization. Now let's write review some code for batch gradient descent just as we did in part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting learning rate, epochs and batch sizes\n",
    "As in part 2, we will need to set a few more parameters which we can adjust for better optimization of the network:\n",
    "- Learning rate: how much the algorithm will move in the negative direction of the gradient, too large and we will likely overshoot a local minimum, too low and no improvement will be seen\n",
    "- Epochs: how many rounds to train the model, too few could be too little for improvement, too many could cause the capture of too much noise in the data\n",
    "- Batch size: the amount of observations that we will be randomly selecting for the optimization of our network, we will average their gradient and move in the direction of the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.02\n",
    "epochs =  300  # stopping rule, how many corrective iterations we will allow\n",
    "batch_size = 500\n",
    "\n",
    "input_dim = XX.shape[1]  # number of variables\n",
    "output_dim = 1  # should be equal to 1 since we are only finding the predicted value for 1 regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewire SGD for binary classification\n",
    "Here is the code which includes the loops for batch gradient descent according to the number of allowed epochs at the specified learning rate with the specific batch size. The only spots here where you should see changes is the loss function and activation functions (along with their respective derivatives). We can then run the code to see if the error does start going down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with average error of 1.48.\n",
      "Epoch 2 with average error of 1.48.\n",
      "Epoch 3 with average error of 1.48.\n",
      "Epoch 4 with average error of 1.48.\n",
      "Epoch 5 with average error of 1.48.\n",
      "Epoch 6 with average error of 1.48.\n",
      "Epoch 7 with average error of 1.48.\n",
      "Epoch 8 with average error of 1.48.\n",
      "Epoch 9 with average error of 1.48.\n",
      "Epoch 10 with average error of 1.48.\n",
      "Epoch 11 with average error of 1.48.\n",
      "Epoch 12 with average error of 1.48.\n",
      "Epoch 13 with average error of 1.48.\n",
      "Epoch 14 with average error of 1.48.\n",
      "Epoch 15 with average error of 1.48.\n",
      "Epoch 16 with average error of 1.48.\n",
      "Epoch 17 with average error of 1.48.\n",
      "Epoch 18 with average error of 1.47.\n",
      "Epoch 19 with average error of 1.47.\n",
      "Epoch 20 with average error of 1.47.\n",
      "Epoch 21 with average error of 1.47.\n",
      "Epoch 22 with average error of 1.47.\n",
      "Epoch 23 with average error of 1.47.\n",
      "Epoch 24 with average error of 1.47.\n",
      "Epoch 25 with average error of 1.47.\n",
      "Epoch 26 with average error of 1.47.\n",
      "Epoch 27 with average error of 1.47.\n",
      "Epoch 28 with average error of 1.47.\n",
      "Epoch 29 with average error of 1.47.\n",
      "Epoch 30 with average error of 1.47.\n",
      "Epoch 31 with average error of 1.47.\n",
      "Epoch 32 with average error of 1.47.\n",
      "Epoch 33 with average error of 1.47.\n",
      "Epoch 34 with average error of 1.47.\n",
      "Epoch 35 with average error of 1.47.\n",
      "Epoch 36 with average error of 1.47.\n",
      "Epoch 37 with average error of 1.47.\n",
      "Epoch 38 with average error of 1.46.\n",
      "Epoch 39 with average error of 1.46.\n",
      "Epoch 40 with average error of 1.46.\n",
      "Epoch 41 with average error of 1.46.\n",
      "Epoch 42 with average error of 1.46.\n",
      "Epoch 43 with average error of 1.46.\n",
      "Epoch 44 with average error of 1.46.\n",
      "Epoch 45 with average error of 1.46.\n",
      "Epoch 46 with average error of 1.45.\n",
      "Epoch 47 with average error of 1.45.\n",
      "Epoch 48 with average error of 1.45.\n",
      "Epoch 49 with average error of 1.45.\n",
      "Epoch 50 with average error of 1.45.\n",
      "Epoch 51 with average error of 1.45.\n",
      "Epoch 52 with average error of 1.45.\n",
      "Epoch 53 with average error of 1.45.\n",
      "Epoch 54 with average error of 1.45.\n",
      "Epoch 55 with average error of 1.45.\n",
      "Epoch 56 with average error of 1.45.\n",
      "Epoch 57 with average error of 1.45.\n",
      "Epoch 58 with average error of 1.45.\n",
      "Epoch 59 with average error of 1.45.\n",
      "Epoch 60 with average error of 1.45.\n",
      "Epoch 61 with average error of 1.45.\n",
      "Epoch 62 with average error of 1.44.\n",
      "Epoch 63 with average error of 1.45.\n",
      "Epoch 64 with average error of 1.45.\n",
      "Epoch 65 with average error of 1.45.\n",
      "Epoch 66 with average error of 1.44.\n",
      "Epoch 67 with average error of 1.44.\n",
      "Epoch 68 with average error of 1.45.\n",
      "Epoch 69 with average error of 1.45.\n",
      "Epoch 70 with average error of 1.44.\n",
      "Epoch 71 with average error of 1.44.\n",
      "Epoch 72 with average error of 1.44.\n",
      "Epoch 73 with average error of 1.44.\n",
      "Epoch 74 with average error of 1.44.\n",
      "Epoch 75 with average error of 1.44.\n",
      "Epoch 76 with average error of 1.44.\n",
      "Epoch 77 with average error of 1.44.\n",
      "Epoch 78 with average error of 1.44.\n",
      "Epoch 79 with average error of 1.43.\n",
      "Epoch 80 with average error of 1.43.\n",
      "Epoch 81 with average error of 1.43.\n",
      "Epoch 82 with average error of 1.43.\n",
      "Epoch 83 with average error of 1.43.\n",
      "Epoch 84 with average error of 1.43.\n",
      "Epoch 85 with average error of 1.43.\n",
      "Epoch 86 with average error of 1.43.\n",
      "Epoch 87 with average error of 1.43.\n",
      "Epoch 88 with average error of 1.43.\n",
      "Epoch 89 with average error of 1.43.\n",
      "Epoch 90 with average error of 1.43.\n",
      "Epoch 91 with average error of 1.43.\n",
      "Epoch 92 with average error of 1.43.\n",
      "Epoch 93 with average error of 1.42.\n",
      "Epoch 94 with average error of 1.42.\n",
      "Epoch 95 with average error of 1.42.\n",
      "Epoch 96 with average error of 1.42.\n",
      "Epoch 97 with average error of 1.42.\n",
      "Epoch 98 with average error of 1.42.\n",
      "Epoch 99 with average error of 1.42.\n",
      "Epoch 100 with average error of 1.43.\n",
      "Epoch 101 with average error of 1.43.\n",
      "Epoch 102 with average error of 1.43.\n",
      "Epoch 103 with average error of 1.42.\n",
      "Epoch 104 with average error of 1.42.\n",
      "Epoch 105 with average error of 1.42.\n",
      "Epoch 106 with average error of 1.42.\n",
      "Epoch 107 with average error of 1.42.\n",
      "Epoch 108 with average error of 1.42.\n",
      "Epoch 109 with average error of 1.42.\n",
      "Epoch 110 with average error of 1.42.\n",
      "Epoch 111 with average error of 1.42.\n",
      "Epoch 112 with average error of 1.43.\n",
      "Epoch 113 with average error of 1.42.\n",
      "Epoch 114 with average error of 1.42.\n",
      "Epoch 115 with average error of 1.42.\n",
      "Epoch 116 with average error of 1.42.\n",
      "Epoch 117 with average error of 1.42.\n",
      "Epoch 118 with average error of 1.42.\n",
      "Epoch 119 with average error of 1.42.\n",
      "Epoch 120 with average error of 1.42.\n",
      "Epoch 121 with average error of 1.42.\n",
      "Epoch 122 with average error of 1.42.\n",
      "Epoch 123 with average error of 1.42.\n",
      "Epoch 124 with average error of 1.42.\n",
      "Epoch 125 with average error of 1.42.\n",
      "Epoch 126 with average error of 1.42.\n",
      "Epoch 127 with average error of 1.43.\n",
      "Epoch 128 with average error of 1.43.\n",
      "Epoch 129 with average error of 1.43.\n",
      "Epoch 130 with average error of 1.43.\n",
      "Epoch 131 with average error of 1.43.\n",
      "Epoch 132 with average error of 1.43.\n",
      "Epoch 133 with average error of 1.42.\n",
      "Epoch 134 with average error of 1.42.\n",
      "Epoch 135 with average error of 1.42.\n",
      "Epoch 136 with average error of 1.42.\n",
      "Epoch 137 with average error of 1.41.\n",
      "Epoch 138 with average error of 1.41.\n",
      "Epoch 139 with average error of 1.41.\n",
      "Epoch 140 with average error of 1.41.\n",
      "Epoch 141 with average error of 1.41.\n",
      "Epoch 142 with average error of 1.40.\n",
      "Epoch 143 with average error of 1.40.\n",
      "Epoch 144 with average error of 1.40.\n",
      "Epoch 145 with average error of 1.40.\n",
      "Epoch 146 with average error of 1.40.\n",
      "Epoch 147 with average error of 1.39.\n",
      "Epoch 148 with average error of 1.40.\n",
      "Epoch 149 with average error of 1.40.\n",
      "Epoch 150 with average error of 1.39.\n",
      "Epoch 151 with average error of 1.39.\n",
      "Epoch 152 with average error of 1.39.\n",
      "Epoch 153 with average error of 1.39.\n",
      "Epoch 154 with average error of 1.38.\n",
      "Epoch 155 with average error of 1.38.\n",
      "Epoch 156 with average error of 1.38.\n",
      "Epoch 157 with average error of 1.38.\n",
      "Epoch 158 with average error of 1.38.\n",
      "Epoch 159 with average error of 1.37.\n",
      "Epoch 160 with average error of 1.37.\n",
      "Epoch 161 with average error of 1.37.\n",
      "Epoch 162 with average error of 1.37.\n",
      "Epoch 163 with average error of 1.36.\n",
      "Epoch 164 with average error of 1.36.\n",
      "Epoch 165 with average error of 1.36.\n",
      "Epoch 166 with average error of 1.37.\n",
      "Epoch 167 with average error of 1.36.\n",
      "Epoch 168 with average error of 1.36.\n",
      "Epoch 169 with average error of 1.36.\n",
      "Epoch 170 with average error of 1.35.\n",
      "Epoch 171 with average error of 1.35.\n",
      "Epoch 172 with average error of 1.35.\n",
      "Epoch 173 with average error of 1.35.\n",
      "Epoch 174 with average error of 1.34.\n",
      "Epoch 175 with average error of 1.34.\n",
      "Epoch 176 with average error of 1.34.\n",
      "Epoch 177 with average error of 1.33.\n",
      "Epoch 178 with average error of 1.33.\n",
      "Epoch 179 with average error of 1.32.\n",
      "Epoch 180 with average error of 1.32.\n",
      "Epoch 181 with average error of 1.32.\n",
      "Epoch 182 with average error of 1.32.\n",
      "Epoch 183 with average error of 1.32.\n",
      "Epoch 184 with average error of 1.33.\n",
      "Epoch 185 with average error of 1.32.\n",
      "Epoch 186 with average error of 1.33.\n",
      "Epoch 187 with average error of 1.32.\n",
      "Epoch 188 with average error of 1.32.\n",
      "Epoch 189 with average error of 1.32.\n",
      "Epoch 190 with average error of 1.31.\n",
      "Epoch 191 with average error of 1.32.\n",
      "Epoch 192 with average error of 1.31.\n",
      "Epoch 193 with average error of 1.31.\n",
      "Epoch 194 with average error of 1.31.\n",
      "Epoch 195 with average error of 1.31.\n",
      "Epoch 196 with average error of 1.30.\n",
      "Epoch 197 with average error of 1.30.\n",
      "Epoch 198 with average error of 1.30.\n",
      "Epoch 199 with average error of 1.30.\n",
      "Epoch 200 with average error of 1.29.\n",
      "Epoch 201 with average error of 1.30.\n",
      "Epoch 202 with average error of 1.29.\n",
      "Epoch 203 with average error of 1.29.\n",
      "Epoch 204 with average error of 1.29.\n",
      "Epoch 205 with average error of 1.29.\n",
      "Epoch 206 with average error of 1.29.\n",
      "Epoch 207 with average error of 1.29.\n",
      "Epoch 208 with average error of 1.30.\n",
      "Epoch 209 with average error of 1.30.\n",
      "Epoch 210 with average error of 1.29.\n",
      "Epoch 211 with average error of 1.29.\n",
      "Epoch 212 with average error of 1.29.\n",
      "Epoch 213 with average error of 1.28.\n",
      "Epoch 214 with average error of 1.28.\n",
      "Epoch 215 with average error of 1.28.\n",
      "Epoch 216 with average error of 1.28.\n",
      "Epoch 217 with average error of 1.28.\n",
      "Epoch 218 with average error of 1.27.\n",
      "Epoch 219 with average error of 1.28.\n",
      "Epoch 220 with average error of 1.28.\n",
      "Epoch 221 with average error of 1.27.\n",
      "Epoch 222 with average error of 1.27.\n",
      "Epoch 223 with average error of 1.27.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224 with average error of 1.27.\n",
      "Epoch 225 with average error of 1.27.\n",
      "Epoch 226 with average error of 1.27.\n",
      "Epoch 227 with average error of 1.27.\n",
      "Epoch 228 with average error of 1.27.\n",
      "Epoch 229 with average error of 1.27.\n",
      "Epoch 230 with average error of 1.26.\n",
      "Epoch 231 with average error of 1.26.\n",
      "Epoch 232 with average error of 1.25.\n",
      "Epoch 233 with average error of 1.25.\n",
      "Epoch 234 with average error of 1.24.\n",
      "Epoch 235 with average error of 1.24.\n",
      "Epoch 236 with average error of 1.24.\n",
      "Epoch 237 with average error of 1.24.\n",
      "Epoch 238 with average error of 1.24.\n",
      "Epoch 239 with average error of 1.25.\n",
      "Epoch 240 with average error of 1.24.\n",
      "Epoch 241 with average error of 1.24.\n",
      "Epoch 242 with average error of 1.24.\n",
      "Epoch 243 with average error of 1.24.\n",
      "Epoch 244 with average error of 1.23.\n",
      "Epoch 245 with average error of 1.23.\n",
      "Epoch 246 with average error of 1.23.\n",
      "Epoch 247 with average error of 1.22.\n",
      "Epoch 248 with average error of 1.22.\n",
      "Epoch 249 with average error of 1.23.\n",
      "Epoch 250 with average error of 1.23.\n",
      "Epoch 251 with average error of 1.23.\n",
      "Epoch 252 with average error of 1.22.\n",
      "Epoch 253 with average error of 1.22.\n",
      "Epoch 254 with average error of 1.22.\n",
      "Epoch 255 with average error of 1.21.\n",
      "Epoch 256 with average error of 1.21.\n",
      "Epoch 257 with average error of 1.21.\n",
      "Epoch 258 with average error of 1.21.\n",
      "Epoch 259 with average error of 1.21.\n",
      "Epoch 260 with average error of 1.20.\n",
      "Epoch 261 with average error of 1.21.\n",
      "Epoch 262 with average error of 1.20.\n",
      "Epoch 263 with average error of 1.20.\n",
      "Epoch 264 with average error of 1.21.\n",
      "Epoch 265 with average error of 1.21.\n",
      "Epoch 266 with average error of 1.21.\n",
      "Epoch 267 with average error of 1.22.\n",
      "Epoch 268 with average error of 1.21.\n",
      "Epoch 269 with average error of 1.20.\n",
      "Epoch 270 with average error of 1.20.\n",
      "Epoch 271 with average error of 1.20.\n",
      "Epoch 272 with average error of 1.20.\n",
      "Epoch 273 with average error of 1.20.\n",
      "Epoch 274 with average error of 1.20.\n",
      "Epoch 275 with average error of 1.20.\n",
      "Epoch 276 with average error of 1.19.\n",
      "Epoch 277 with average error of 1.19.\n",
      "Epoch 278 with average error of 1.18.\n",
      "Epoch 279 with average error of 1.19.\n",
      "Epoch 280 with average error of 1.19.\n",
      "Epoch 281 with average error of 1.19.\n",
      "Epoch 282 with average error of 1.18.\n",
      "Epoch 283 with average error of 1.19.\n",
      "Epoch 284 with average error of 1.19.\n",
      "Epoch 285 with average error of 1.18.\n",
      "Epoch 286 with average error of 1.19.\n",
      "Epoch 287 with average error of 1.20.\n",
      "Epoch 288 with average error of 1.20.\n",
      "Epoch 289 with average error of 1.19.\n",
      "Epoch 290 with average error of 1.19.\n",
      "Epoch 291 with average error of 1.19.\n",
      "Epoch 292 with average error of 1.19.\n",
      "Epoch 293 with average error of 1.19.\n",
      "Epoch 294 with average error of 1.19.\n",
      "Epoch 295 with average error of 1.20.\n",
      "Epoch 296 with average error of 1.19.\n",
      "Epoch 297 with average error of 1.19.\n",
      "Epoch 298 with average error of 1.19.\n",
      "Epoch 299 with average error of 1.20.\n",
      "Epoch 300 with average error of 1.20.\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "while iteration < epochs:\n",
    "\n",
    "    # Process one batch of random observations per epoch/iteration\n",
    "    random_batch = random.sample(range(0, XX.shape[0]), batch_size)  # choose observations for random batch\n",
    "\n",
    "    HiddenToOutput_gradient_list = [] # to store the gradients based on each observation in batch\n",
    "    InputToHidden_gradient_list = []\n",
    "\n",
    "    # Update weights and biases one at a time with each random observation's error (all steps before, just in a loop together)\n",
    "    for obs in random_batch:\n",
    "\n",
    "        # Select feature values and target value for random observation\n",
    "        X = np.array(XX[obs]).reshape((input_dim, 1))\n",
    "        y = Y[obs].reshape((1, 1))\n",
    "\n",
    "        # Compute the forward pass through the network all the way up to the final output\n",
    "        Z_hidden = np.dot(W_0, X) + B_0\n",
    "        H = tanH(Z_hidden)\n",
    "        Z_output = np.dot(W_1, H) + B_1\n",
    "        y_hat = sigmoid(Z_output)  # final activation is sigmoid instead of linear\n",
    "\n",
    "        # Gradient for the weights between hidden and output layers\n",
    "        gradient_HiddenToOutput = bce_derivative(Y[obs], y_hat) * sigmoid_derivative(y_hat) # use derivative of BCE loss and sigmoid\n",
    "        pred_errors = gradient_HiddenToOutput\n",
    "\n",
    "        # Gradient for the weights between input and hidden layers\n",
    "        gradient_InputToHidden = np.dot(W_1.T, pred_errors) * tanH_derivative(Z_hidden)\n",
    "        \n",
    "        HiddenToOutput_gradient_list.append(gradient_HiddenToOutput)\n",
    "        InputToHidden_gradient_list.append(gradient_InputToHidden)\n",
    "\n",
    "    mean_gradient_HiddenToOutput = np.mean(HiddenToOutput_gradient_list, axis=0) # average optimal movemnt over batch\n",
    "    mean_gradient_InputToHidden = np.mean(InputToHidden_gradient_list, axis=0)\n",
    "    \n",
    "    # Update biases according to learning rate and gradient\n",
    "    B_1 -= learningRate * mean_gradient_HiddenToOutput\n",
    "    B_0 -= learningRate * mean_gradient_InputToHidden\n",
    "\n",
    "    # Update weights according to learning rate and gradient\n",
    "    W_1 -= learningRate * np.dot(mean_gradient_HiddenToOutput, np.transpose(Z_hidden))\n",
    "    W_0 -= learningRate * np.dot(mean_gradient_InputToHidden, np.transpose(X))\n",
    "    \n",
    "    # Check how well the model does on all observations by passing them through a forward pass with most up to date weights\n",
    "    XX_reshaped = np.array(XX).reshape((inputLayer_size, n))  # all observations\n",
    "    Y_reshaped = Y.reshape((outputLayer_size, n))\n",
    "    Z_hidden = np.dot(W_0, XX_reshaped) + B_0  # first hidden layer inputs\n",
    "    H = tanH(Z_hidden)  # hidden layer output (after activation)\n",
    "    Z_output = np.dot(W_1, H) + B_1  # input to output layer\n",
    "    Y_hat = sigmoid(Z_output) # Sigmoid output instead of linear\n",
    "    \n",
    "    # Calculate loss for entire model\n",
    "    iteration_loss = bce(Y_reshaped, Y_hat) # Here are we are using BCE\n",
    "    loss_log.append(iteration_loss)\n",
    "\n",
    "    # Development of the loss as average over observation-level losses\n",
    "    print(f'Epoch {iteration+1} with average error of {iteration_loss:.2f}.')\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this simple network, we can clearly see progress is being made! Not only that, hopefully you can now see the large similarities between classification and regression networks. They are highly related to each other, you only need to pay attention to: output layer size, final activation function and the loss function. Other than that, they are basically the same thing. There are of course many other powerful settings in more complex packages which you would be wise to explore. Creating a good neural network for a real task takes quite a lot of time and effort. This can be saved by using all the tools you have available to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with multi-class problems\n",
    "Now that you have an understanding on how binary classification works, you should find the transition to multi-class problems to be not too difficult. There are really only a few things to keep in mind regarding the output layer size, the recommended final activations and the recommended loss functions. Other than that, the rest is pretty much identical.\n",
    "\n",
    "### Output Layer\n",
    "- Your output layer will need one node per class.\n",
    "\n",
    "\n",
    "### Final Activation\n",
    "- For **single label multi-class problems**: softmax as all class probabilities should sum to 1 since only one label is possible.\n",
    "- For **multi-label multi-class problems**: sigmoid as there is no restriction on the sum of all probabilities since multiple labels are possible. \n",
    "\n",
    "\n",
    "### Loss functions\n",
    "- For **single label multi-class problems**, you should use categorical cross-entropy or the generalized version of the equation for $ L $ above (categorical cross-entropy).\n",
    "- For **multi-label multi-class problems**, you can use binary cross-entropy again because any label has its own probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheat sheet for each scenario:\n",
    "\n",
    "For **regression problems**:\n",
    "- Make sure your output is 1 variable with any range\n",
    "- Output layer should be size 1\n",
    "- Output could be any number\n",
    "- Final activation function: linear or ReLU\n",
    "- Most popular loss function: MSE or MAE\n",
    "\n",
    "For **binary classification problem with target coded as a single dummy vector**:\n",
    "- Make sure your output is a single binary variable with values 0 and 1\n",
    "- Output layer should be size 1\n",
    "- Output will range from 0 to 1 and represent the probability of being class 1\n",
    "- Final activation function: sigmoid\n",
    "- Loss function: binary cross entropy\n",
    "\n",
    "For **binary classification problem with target coded as two complementary dummies**:\n",
    "- Make sure your output is two binary variables representing probabilities of class 0 and class 1\n",
    "- Output layer should be size 2\n",
    "- Output nodes will range from 0 to 1 and represent the probability of being class 0 and 1 respectively\n",
    "- Final activation function: softmax\n",
    "- Loss function: binary cross entropy\n",
    "\n",
    "For **multiple possible classes but strictly 1 class per observation (single label)**:\n",
    "- Make sure your output size is equivalent to the number of classes\n",
    "- Each value for outputs must be between 0 and 1\n",
    "- Output layer should be equivalent to the number of classes\n",
    "- Output will range for each class from 0 to 1 and represent the probability of being that class\n",
    "- Final activation: softmax\n",
    "- Loss function: categorical cross entropy\n",
    "\n",
    "For **multiple possible classes and multiple classes per observation are possible (multi-label)**:\n",
    "- Make sure your output size is equivalent to the number of classes\n",
    "- Each value for outputs must be between 0 and 1\n",
    "- Output layer should be equivalent to the number of classes\n",
    "- Output will range for each class from 0 to 1 and represent the probability of being that class\n",
    "- Final activation: sigmoid\n",
    "- Loss function: binary cross entropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
